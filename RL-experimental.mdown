# Experimental Papers

* *Deep Reinforcement Learning and the Deadly Triad* (2018)

This paper studies the convergence behavior of the Q-value, and concluded that:

1) action value can commonly exhibit exponential initial growth, yet still subsequently recover to normal.

2) The instability in standard epsilon-greedy scheme can be reduced by bootstrapping on a separate network & reducing overestimation bias.

(Note: See "conservative Q-learning" for a recent(2020) solution; the overestimation bias is caused by the max operator. hint: E(max(x))>=max(E(x)))

3) Longer bootstrap length reduces the prevalence of instabilities.

4) Unrealisitic value estimation (positively) correlates with poor performance. For those cases where agents have wrong estimation yet good performance, the ranking of the values are roughly preserved.

* *Deep Reinforcement Learning that Matters* (2017)

This paper finishes many experiments yet concluded with a pessimism summary:

1) Hyperparameters are very important;

2) Variance across trials are large;

3) Even different realization of the same algorithm may vary greatly on a given environment.

* *Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO* (2020)

It is interesting that PPO without optimizations actually has poor performance and no edge over TRPO. Some important notes:

1) Common optimization of PPO includes value function clipping, reward scaling, orthogonal initialization & layer scaling, Adam LR annealing, observation normalization & clipping, tanh activation and gradient clipping.

2) It is difficult to attribute success to different aspects of policy gradient methods without careful analysis.

3) PPO and TRPO with code optimization can maintain a certain average KL-divergence (thus remain in the "trust region"), but PPO without code optimization may not.


* *Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?* (2018)

This paper questions DPG algorithms from principle and calls for a multi-perspective remark for RL algorithms (e.g. the ability to capture the structure of real-life problems).

The paper suggests that the normal sampling size, the gradient and the landscape of the value function are actually far different from ground truth, and the gradient varies greatly among multiple runs; to eliminate the difference would require ~1000x more samples.


* *Diagnosing Bottlenecks in Deep Q-learning Algorithms* (ICLR 19') 

一篇比较全面地分析Q-learning算法在实际神经网络使用中情况的文章。除了很实用之外，文章使用的三种用于分析的策略（exact，sample和replay-FQI）也提供了一个非常好的insight。 

Smaller architectures introduce significant bias in the learning process. This gap may be due to the fact that when the target is bootstrapped, we must be able to represent all Q-function along the path to the solution, and not just the final result. 

Higher sample count leads to improved learning speed and a better final solution, confirming our hypothesis that overfitting has a significant effect on the performance of Q-learning. 

Replay buffers and early stopping can be used to mitigate the effects of overfitting. nonstationarities in both distributions and target values, when isolated, do not cause significant stability issues. Instead, other factors such as sampling error and function approximation appear to have more significant effects on performance. 

文章还提出了更好的sampling方法：Adversarial Feature Matching。
































## Introduction

Markdown is a plain text formatting syntax.

Paragraphs are separated by empty lines.

## Heading 2

### Heading 3

#### Heading 4

##### Heading 5

###### Heading 6

## Character Styles

These spans result in 'em' tags:

- *single asterisks*
- _single underscores_

These spans result in 'strong' tags:

- **double asterisks**
- __double underscores__

These spans result in 'del' tags:

- ~~double tildes~~

## Links and Images

This is an [example inline link](https://www.actiprosoftware.com "Actipro Software") with tooltip text specified.
[This link](https://www.actiprosoftware.com) has no tooltip text specified.

URLs and e-mail addresses can be turned into links by enclosing them in angle braces:

- <https://www.actiprosoftware.com>  
- <support@microsoft.com>

[This link](#markdown-document) links to the first heading in this document via custom ID.

## Images

This is an example of an image:

![Image](https://www.microsoft.com/favicon.ico)

This is an example of an image with a link:

[![Image](https://www.google.com/favicon.ico)](https://www.google.com)

## Blockquotes

Markdown said:

> This is the first level of quoting.
>
> > This is a nested blockquote.
>
> Back to the first level.

## Lists

Unordered list using minus signs (-):

- Step 1
- Step 2
- Step 3
  - Step 3a
  - Step 3b
  - Step 3c

Unordered list using plus signs (+):

+ Step 1
+ Step 2
+ Step 3
  + Step 3a
  + Step 3b
  + Step 3c

Unordered list using asterisks (*):

* Step 1
* Step 2
* Step 3
  * Step 3a
  * Step 3b
  * Step 3c

Ordered list:

1. Step 1
1. Step 2
1. Step 3
    1. Step 3a
    1. Step 3b
    1. Step 3c

Nested (unordered within ordered) list:

1. Step 1
1. Step 2
1. Step 3
    - Step 3a
    - Step 3b
    - Step 3c

Definition list:

Term #1
: This is the definition of term #1.

Term #2
: This is the definition of term #2.

## Code Blocks

Inline `code` can be delimited with characters.

This code block is fenced with three backticks and has its language specified:

```javascript
var oldUnload = window.onbeforeunload;
window.onbeforeunload = function() {
    saveCoverage();
    if (oldUnload) {
        return oldUnload.apply(this, arguments);
    }
};
```

This code block is fenced with three tildes and has its language specified:

~~~ruby
require 'redcarpet'
markdown = Redcarpet.new("Hello World!")
puts markdown.to_html
~~~

This code block is created by indenting the code, but no language can be specified:

    var foo = 1;

## Tables

| Fruit  | Color  |
|--------|--------|
| Apples | Red    |
| Grapes | Purple |
| Lemons | Yellow |

## Horizontal Rules

Horizontal rules are formed by placing three or more hyphens, asterisks, or underscores on a line by themselves.

---

***

___

## HTML Tags

<strong>HTML tags</strong> can optionally be used in <em>Markdown</em>.

## Special Characters

Unescaped:
\ ` * _ { } [ ] ( ) # + - . !

Backslash-Escaped:
\\ \` \* \_ \{ \} \[ \] \( \) \# \+ \- \. \!
