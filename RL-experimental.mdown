# Experimental Papers

* *Deep Reinforcement Learning and the Deadly Triad* (2018)

This paper studies the convergence behavior of the Q-value, and concluded that:

1) action value can commonly exhibit exponential initial growth, yet still subsequently recover to normal.

2) The instability in standard epsilon-greedy scheme can be reduced by bootstrapping on a separate network & reducing overestimation bias.

(Note: See "conservative Q-learning" for a recent(2020) solution; the overestimation bias is caused by the max operator. hint: E(max(x))>=max(E(x)))

3) Longer bootstrap length reduces the prevalence of instabilities.

4) Unrealisitic value estimation (positively) correlates with poor performance. For those cases where agents have wrong estimation yet good performance, the ranking of the values are roughly preserved.

* *Deep Reinforcement Learning that Matters* (2017)

This paper finishes many experiments yet concluded with a pessimism summary:

1) Hyperparameters are very important;

2) Variance across trials are large;

3) Even different realization of the same algorithm may vary greatly on a given environment.

* *Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO* (2020)

It is interesting that PPO without optimizations actually has poor performance and no edge over TRPO. Some important notes:

1) Common optimization of PPO includes value function clipping, reward scaling, orthogonal initialization & layer scaling, Adam LR annealing, observation normalization & clipping, tanh activation and gradient clipping.

2) It is difficult to attribute success to different aspects of policy gradient methods without careful analysis.

3) PPO and TRPO with code optimization can maintain a certain average KL-divergence (thus remain in the "trust region"), but PPO without code optimization may not.


* *Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?* (2018)

This paper questions DPG algorithms from principle and calls for a multi-perspective remark for RL algorithms (e.g. the ability to capture the structure of real-life problems).

The paper suggests that the normal sampling size, the gradient and the landscape of the value function are actually far different from ground truth, and the gradient varies greatly among multiple runs; to eliminate the difference would require ~1000x more samples.


* *Diagnosing Bottlenecks in Deep Q-learning Algorithms* (ICLR 19') 

Smaller architectures introduce significant bias in the learning process. This gap may be due to the fact that when the target is bootstrapped, we must be able to represent all Q-function along the path to the solution, and not just the final result. 

Higher sample count leads to improved learning speed and a better final solution, confirming our hypothesis that overfitting has a significant effect on the performance of Q-learning. 

Replay buffers and early stopping can be used to mitigate the effects of overfitting. nonstationarities in both distributions and target values, when isolated, do not cause significant stability issues. Instead, other factors such as sampling error and function approximation appear to have more significant effects on performance. 

The paper also proposes a better sampling method: Adversarial Feature Matching.





























